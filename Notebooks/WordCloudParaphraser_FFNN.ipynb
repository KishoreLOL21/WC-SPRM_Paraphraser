{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy pandas tensorflow scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "IrlnM8OtHxwp",
        "outputId": "07e64b5a-77e0-4d4e-c515-9e326d093c70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.25.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.70.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Reshape\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "data = pd.read_csv('/content/parent-synonym-embedding.csv')  # Replace with your dataset path\n",
        "\n",
        "# Step 2: Convert embeddings from string to numpy arrays\n",
        "data['Parent Embedding'] = data['Parent Embedding'].apply(lambda x: np.array(eval(x)))\n",
        "for i in range(1, 6):\n",
        "    data[f'Synonym-{i} Embedding'] = data[f'Synonym-{i} Embedding'].apply(lambda x: np.array(eval(x)))\n",
        "\n",
        "# Step 3: Prepare input (X) and output (y) data\n",
        "X = np.array(data['Parent Embedding'].tolist())\n",
        "y = np.array([row[[f'Synonym-{i} Embedding' for i in range(1, 6)]].values.tolist() for _, row in data.iterrows()])\n",
        "\n",
        "# Step 4: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 5: Define the FFNN model\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))  # Input layer\n",
        "model.add(Dense(64, activation='relu'))  # Hidden layer\n",
        "model.add(Dense(y_train.shape[1] * X_train.shape[1], activation='linear'))   # Output layer with correct shape\n",
        "model.add(Reshape((y_train.shape[1], X_train.shape[1]))) # Reshape to (batch_size, 5, 20)\n",
        "\n",
        "# Step 6: Compile the model\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "# Step 7: Train the model\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Step 8: Evaluate the model on the test set\n",
        "loss, mae = model.evaluate(X_test, y_test)\n",
        "print(f'Test Loss: {loss}, Test MAE: {mae}')\n",
        "\n",
        "# Step 9: Save the trained model\n",
        "model.save('paraphrase_model.h5', save_format='tf') # Changed to model.save and specify format\n",
        "print(\"Model saved as 'paraphrase_model.h5'\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGP8US7WMhmj",
        "outputId": "a5738401-ff05-49a3-9f18-3ac36f30ad68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 0.0513 - mae: 0.1963 - val_loss: 0.0474 - val_mae: 0.1901\n",
            "Epoch 2/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0465 - mae: 0.1881 - val_loss: 0.0447 - val_mae: 0.1846\n",
            "Epoch 3/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0435 - mae: 0.1817 - val_loss: 0.0421 - val_mae: 0.1783\n",
            "Epoch 4/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0408 - mae: 0.1746 - val_loss: 0.0394 - val_mae: 0.1708\n",
            "Epoch 5/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0376 - mae: 0.1659 - val_loss: 0.0365 - val_mae: 0.1621\n",
            "Epoch 6/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0336 - mae: 0.1542 - val_loss: 0.0334 - val_mae: 0.1526\n",
            "Epoch 7/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0306 - mae: 0.1450 - val_loss: 0.0299 - val_mae: 0.1427\n",
            "Epoch 8/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0268 - mae: 0.1341 - val_loss: 0.0264 - val_mae: 0.1325\n",
            "Epoch 9/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0237 - mae: 0.1249 - val_loss: 0.0232 - val_mae: 0.1223\n",
            "Epoch 10/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0201 - mae: 0.1138 - val_loss: 0.0203 - val_mae: 0.1132\n",
            "Epoch 11/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0177 - mae: 0.1059 - val_loss: 0.0179 - val_mae: 0.1057\n",
            "Epoch 12/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0152 - mae: 0.0977 - val_loss: 0.0162 - val_mae: 0.0999\n",
            "Epoch 13/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0138 - mae: 0.0928 - val_loss: 0.0147 - val_mae: 0.0949\n",
            "Epoch 14/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0123 - mae: 0.0875 - val_loss: 0.0135 - val_mae: 0.0910\n",
            "Epoch 15/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0113 - mae: 0.0836 - val_loss: 0.0126 - val_mae: 0.0876\n",
            "Epoch 16/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0104 - mae: 0.0796 - val_loss: 0.0119 - val_mae: 0.0846\n",
            "Epoch 17/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0096 - mae: 0.0763 - val_loss: 0.0113 - val_mae: 0.0821\n",
            "Epoch 18/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0088 - mae: 0.0725 - val_loss: 0.0107 - val_mae: 0.0799\n",
            "Epoch 19/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0086 - mae: 0.0718 - val_loss: 0.0104 - val_mae: 0.0781\n",
            "Epoch 20/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0082 - mae: 0.0699 - val_loss: 0.0099 - val_mae: 0.0764\n",
            "Epoch 21/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0077 - mae: 0.0675 - val_loss: 0.0096 - val_mae: 0.0749\n",
            "Epoch 22/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0074 - mae: 0.0663 - val_loss: 0.0093 - val_mae: 0.0733\n",
            "Epoch 23/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0071 - mae: 0.0644 - val_loss: 0.0091 - val_mae: 0.0723\n",
            "Epoch 24/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0068 - mae: 0.0629 - val_loss: 0.0088 - val_mae: 0.0708\n",
            "Epoch 25/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0065 - mae: 0.0613 - val_loss: 0.0086 - val_mae: 0.0700\n",
            "Epoch 26/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0065 - mae: 0.0611 - val_loss: 0.0084 - val_mae: 0.0688\n",
            "Epoch 27/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0062 - mae: 0.0592 - val_loss: 0.0083 - val_mae: 0.0684\n",
            "Epoch 28/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0060 - mae: 0.0583 - val_loss: 0.0080 - val_mae: 0.0671\n",
            "Epoch 29/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0059 - mae: 0.0574 - val_loss: 0.0079 - val_mae: 0.0664\n",
            "Epoch 30/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0057 - mae: 0.0565 - val_loss: 0.0078 - val_mae: 0.0656\n",
            "Epoch 31/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.0057 - mae: 0.0560 - val_loss: 0.0076 - val_mae: 0.0648\n",
            "Epoch 32/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0056 - mae: 0.0554 - val_loss: 0.0075 - val_mae: 0.0643\n",
            "Epoch 33/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0054 - mae: 0.0545 - val_loss: 0.0074 - val_mae: 0.0637\n",
            "Epoch 34/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0053 - mae: 0.0536 - val_loss: 0.0074 - val_mae: 0.0632\n",
            "Epoch 35/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0051 - mae: 0.0527 - val_loss: 0.0072 - val_mae: 0.0625\n",
            "Epoch 36/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - loss: 0.0052 - mae: 0.0527 - val_loss: 0.0072 - val_mae: 0.0621\n",
            "Epoch 37/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - loss: 0.0051 - mae: 0.0519 - val_loss: 0.0071 - val_mae: 0.0616\n",
            "Epoch 38/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0049 - mae: 0.0514 - val_loss: 0.0070 - val_mae: 0.0612\n",
            "Epoch 39/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0049 - mae: 0.0511 - val_loss: 0.0070 - val_mae: 0.0609\n",
            "Epoch 40/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 0.0049 - mae: 0.0505 - val_loss: 0.0069 - val_mae: 0.0604\n",
            "Epoch 41/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0049 - mae: 0.0502 - val_loss: 0.0069 - val_mae: 0.0602\n",
            "Epoch 42/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0047 - mae: 0.0495 - val_loss: 0.0068 - val_mae: 0.0599\n",
            "Epoch 43/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0047 - mae: 0.0495 - val_loss: 0.0068 - val_mae: 0.0597\n",
            "Epoch 44/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0047 - mae: 0.0490 - val_loss: 0.0068 - val_mae: 0.0593\n",
            "Epoch 45/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0047 - mae: 0.0488 - val_loss: 0.0067 - val_mae: 0.0589\n",
            "Epoch 46/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0047 - mae: 0.0488 - val_loss: 0.0067 - val_mae: 0.0589\n",
            "Epoch 47/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0046 - mae: 0.0481 - val_loss: 0.0066 - val_mae: 0.0586\n",
            "Epoch 48/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0046 - mae: 0.0482 - val_loss: 0.0066 - val_mae: 0.0584\n",
            "Epoch 49/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0045 - mae: 0.0474 - val_loss: 0.0065 - val_mae: 0.0579\n",
            "Epoch 50/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0045 - mae: 0.0474 - val_loss: 0.0066 - val_mae: 0.0579\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0065 - mae: 0.0577\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:The `save_format` argument is deprecated in Keras 3. We recommend removing this argument as it can be inferred from the file path. Received: save_format=tf\n",
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.006528886500746012, Test MAE: 0.05760196968913078\n",
            "Model saved as 'paraphrase_model.h5'\n"
          ]
        }
      ]
    },
    {
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sys\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Step 1: Load the saved model\n",
        "print(\"Loading model...\", flush=True)\n",
        "model = load_model('paraphrase_model.h5', compile=False)  # Load without compiling\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mae'])  # Recompile\n",
        "print(\"Model loaded successfully.\", flush=True)\n",
        "\n",
        "# Step 2: Load word_vectors.csv (parent words and their embeddings)\n",
        "print(\"Loading word_vectors.csv...\", flush=True)\n",
        "word_vectors_df = pd.read_csv('/content/word_vectors.csv')\n",
        "print(f\"Columns in word_vectors.csv: {word_vectors_df.columns}\", flush=True)\n",
        "\n",
        "embedding_column = word_vectors_df.columns[1]  # Assuming embedding is the second column\n",
        "# Removed the unnecessary apply call that was causing the error\n",
        "# word_vectors_df[embedding_column] = word_vectors_df[embedding_column].apply(lambda x: np.array(eval(x)))\n",
        "word_vectors = dict(zip(word_vectors_df['word'], word_vectors_df[embedding_column]))\n",
        "print(word_vectors)\n",
        "\n",
        "# Step 3: Load synonyms.csv (complete vocabulary of synonyms and their embeddings)\n",
        "print(\"Loading synonyms.csv...\", flush=True)\n",
        "synonyms_df = pd.read_csv('/content/synonyms.csv', header=None)\n",
        "synonyms_df.rename(columns={0: 'Synonym'}, inplace=True)\n",
        "\n",
        "# Extract embeddings as NumPy array and convert properly, remove the applymap call\n",
        "synonym_embeddings = synonyms_df.iloc[:, 1:].values\n",
        "synonyms = dict(zip(synonyms_df['Synonym'], synonym_embeddings))\n",
        "print(f\"Loaded {len(synonyms)} synonyms.\", flush=True)\n",
        "\n",
        "# Step 4: Function to find the closest synonym word\n",
        "def find_closest_synonym(embedding, synonyms):\n",
        "    closest_word = None\n",
        "    max_similarity = -1\n",
        "\n",
        "    for word, vector in synonyms.items():\n",
        "        similarity = cosine_similarity([embedding], [vector])[0][0]\n",
        "        if similarity > max_similarity:\n",
        "            max_similarity = similarity\n",
        "            closest_word = word\n",
        "\n",
        "    print(f\"Closest synonym found: {closest_word} (Similarity: {max_similarity})\", flush=True)\n",
        "    return closest_word\n",
        "\n",
        "# Step 5: Function to paraphrase a sentence\n",
        "def paraphrase_sentence(sentence, word_vectors, synonyms, model):\n",
        "    print(f\"Paraphrasing sentence: {sentence}\", flush=True)\n",
        "    words = sentence.split()\n",
        "    paraphrased_sentence = []\n",
        "\n",
        "    for word in words:\n",
        "        if word in word_vectors:\n",
        "            # Step 1: Get the parent embedding\n",
        "            parent_embedding = word_vectors[word]\n",
        "            print(f\"Embedding for '{word}': {parent_embedding}\", flush=True)\n",
        "\n",
        "            try:\n",
        "                # Step 2: Predict the synonym embedding\n",
        "                predicted_synonym_embedding = model.predict(np.array([parent_embedding]))[0]\n",
        "                print(f\"Predicted embedding for '{word}': {predicted_synonym_embedding}\", flush=True)\n",
        "\n",
        "                # Step 3: Find the closest synonym word\n",
        "                closest_synonym = find_closest_synonym(predicted_synonym_embedding, synonyms)\n",
        "                paraphrased_sentence.append(closest_synonym)\n",
        "            except Exception as e:\n",
        "                print(f\"Error predicting synonym for '{word}': {e}\", flush=True)\n",
        "                paraphrased_sentence.append(word)  # Keep original word on failure\n",
        "        else:\n",
        "            print(f\"Word '{word}' not found in word vectors, keeping it unchanged.\", flush=True)\n",
        "            paraphrased_sentence.append(word)\n",
        "\n",
        "    paraphrased_text = ' '.join(paraphrased_sentence)\n",
        "    print(f\"Final paraphrased sentence: {paraphrased_text}\", flush=True)\n",
        "    return paraphrased_text\n",
        "\n",
        "# Step 6: Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Starting paraphrasing process...\", flush=True)\n",
        "\n",
        "    # Input sentence\n",
        "    sentence = \"The quick brown fox jumped over the huge wall\"\n",
        "\n",
        "    # Paraphrase the sentence\n",
        "    paraphrased_sentence = paraphrase_sentence(sentence, word_vectors, synonyms, model)\n",
        "\n",
        "    print(f'Original Sentence: {sentence}', flush=True)\n",
        "    print(f'Paraphrased Sentence: {paraphrased_sentence}', flush=True)\n",
        "\n",
        "    sys.stdout.flush()  # Force output if running in buffered environments"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SjqzU-KQ3_6",
        "outputId": "1b5f386a-1569-434d-9f3c-daca92681d3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model...\n",
            "Model loaded successfully.\n",
            "Loading word_vectors.csv...\n",
            "Columns in word_vectors.csv: Index(['word', 'vector'], dtype='object')\n",
            "{-0.32571685: 0.2370348, -0.2928181: 0.114224516, -0.23087312: 0.03726288, 0.079329096: -0.063414566, 0.07696159: -0.29203844, -0.25135127: 0.22717102, -0.19286314: 0.2383442, -0.19125912: 0.23251452, -0.1575665: 0.29443133, -0.34171999: -0.2839549, -0.24350148: -0.19983734, -0.25435737: -0.26340753, -0.03231681: 0.2888182, 0.04287957: 0.2638361, 0.29722205: -0.30206951, -0.06936145: 0.2859176, -0.10753195: -0.28023738, 0.25185424: -0.19522798, -0.29206997: 0.12095589, -0.22663373: -0.23525415, 0.21334283: 0.22564988, -0.31323645: -0.20591897, 0.17965874: 0.17990425, 0.40626368: -0.30814192, 0.15974215: 0.31327423, 0.06825441: 0.14610286, 0.2754313: -0.3585307, -0.26235715: -0.27208456, -0.022279005: 0.28042865, 0.36751276: -0.27252883, 0.1389837: 0.25283426, 0.34448788: -0.26373684, 0.039179407: 0.24362923, -0.14345916: -0.088127986, 0.1473631: 0.25790694, 0.248198: 0.21610403, 0.31468862: -0.19657674, 0.042660326: 0.2689408, -0.33511505: -0.22712012, 0.047361035: 0.11290691, -0.13584746: 0.08419302, 0.23654361: 0.23091674, -0.03691619: 0.09313545, -0.25620684: 0.03505857, 0.2611691: -0.28315634, -0.35919285: -0.26312307, 0.3666401: -0.2897478, -0.28087735: -0.3334977, -0.09749698: 0.08464489, 0.26198035: -0.18927577, 0.18919091: 0.26055497, -0.29629418: -0.36474136, 0.3057726: -0.31707117, -0.19439112: -0.26900735, -0.32963386: -0.24973744, 0.25931922: -0.10566199, -0.22960699: -0.30811277, -0.046766955: -0.29847392, -0.2691944: -0.2325587, -0.04397566: -0.3163496, -0.14269285: -0.03694638, 0.05319469: 0.24528226, -0.195242: 0.317097, -0.027018428: -0.07613389, 0.2934538: -0.29764622, -0.14921665: 0.27334055, 0.23223709: -0.22190696, 0.28106174: -0.28248784, 0.21092619: -0.31202477, 0.210272: -0.26136276, 0.20247468: -0.29848996, -0.018256385: 0.13400726, 0.20787224: -0.10187695, 0.09386933: -0.19218132, 0.059741333: 0.20162888, 0.27519846: -0.22946174, -0.2858112: -0.31965268, 0.058901466: -0.1760413, 0.17758003: 0.3924678, 0.099808246: -0.3547266, 0.024406463: 0.21782137, -0.110293396: -0.17444853, 0.12066771: -0.24336873, 0.2206132: 0.25891733, 0.24811587: -0.16529945, 0.1874446: -0.2651053, -0.16583015: 0.33580294, 0.27935013: 0.29111695, 0.1580732: -0.032343734, 0.121933974: 0.007184942, 0.023309529: -0.32959446, -0.24395192: 0.17753364, 0.1087255: -0.027257368, -0.19583131: 0.16097292, 0.36376387: -0.18747064, -0.29254293: -0.3328858, 0.30864185: 0.10464127, -0.050080545: -0.37664378, 0.1759845: 0.13462994, 0.34994093: 0.17785007, 0.18458512: 0.20849153, 0.008281992: -0.2651068, -0.054933313: 0.3326403, 0.2853122: -0.20226784, -0.19611265: -0.3519521, 0.30871123: 0.23169692, 0.10761363: 0.31338206, 0.18587074: 0.3213567, 0.15463485: 0.32240048, -0.036692966: 0.39841232, 0.25894323: 0.24203287, 0.23440203: 0.28665435, -0.14794557: -0.39505515, 0.2087127: 0.047872227, 0.07335812: 0.33271232, 0.23107718: 0.32698447, 0.2721331: 0.08179156, -0.054391466: 0.37147492, -0.20467243: 0.38889894, -0.2312389: -0.2230308, -0.025582518: 0.32670087, -0.24542253: -0.348961, -0.09693051: 0.17204793, -0.2256737: -0.34983742, 0.4756901: -0.013281271, -0.29412735: 0.41883174, 0.36358884: 0.07654886, 0.29591742: 0.16503103, -0.34655917: -0.22305493, 0.14140125: -0.22591738, -0.26465237: 0.28910464, -0.28550392: -0.33380535, 0.339675: 0.24737555, -0.2360139: -0.27961257, -0.3079627: 0.16692807, -0.19423646: -0.032142438, -0.33584556: -0.19027376, 0.041551642: 0.25959817, -0.29682645: 0.33909786, -0.32929075: -0.13369021, -0.32252046: 0.32849595, 0.3177395: 0.26011032, -0.33507854: 0.28259498, 0.223707: -0.25526914, 0.2860778: 0.19842403, 0.19157468: 0.33398044, 0.30052203: 0.2604845, -0.2835323: -0.31458837, 0.22757211: -0.24640556, 0.10015836: 0.10123032, 0.23394233: 0.34789413, -0.0293372: 0.35216412, -0.12210515: 0.27897567, 0.0074812486: -0.006698903, -0.23468462: -0.22506618, -0.27262893: -0.15853132, -0.32037887: 0.30896056, 0.24729824: 0.23783034, -0.34592283: 0.31302547, -0.24912328: -0.042775877, 0.20416352: 0.24316727, 0.28133404: 0.30428383, 0.25066155: 0.0724323, -0.2755042: 0.04817629, 0.09196668: 0.06279561, 0.018803438: -0.081945784, -0.19528706: -0.020785619, -0.12272797: 0.014463996, -0.34509116: 0.29456005, -0.258471: 0.31225193, -0.30760998: 0.28492457, 0.13284613: 0.17782806, -0.26016086: 0.13094975, -0.2138177: 0.15070403, -0.114252865: 0.26928338, 0.18138477: 0.10494934, 0.056358825: 0.18787962, 0.024004154: -0.07750038, 0.030662065: 0.19934992, 0.0056000105: 0.010513906, -0.029786818: 0.25867605, -0.25329092: 0.31827313, 0.22161086: 0.31236643, -0.33238244: 0.2753478, -0.21139549: 0.1568647, -0.23291707: 0.28672877, 0.22014223: 0.26015887, -0.32346904: -0.22140226, -0.29003698: 0.18959376, -0.30803806: 0.20036675, -0.3639912: 0.22463208, -0.3299299: 0.017938087, 0.1947004: 0.2114817, -0.30222324: 0.048055522, 0.28546718: 0.27020037, 0.24450469: -0.19041304, 0.11063709: 0.29941183, -0.050744522: -0.10544843, -0.37658373: -0.049982417, -0.29503116: -0.1630241, -0.29772294: 0.2526774, 0.07792572: 0.20097744, -0.24356665: 0.39773262, 0.28602964: 0.26975164, 0.2872292: 0.27963614, -0.20451602: -0.21570505, 0.22919936: 0.32919455, 0.09674695: -0.08490001, 0.29638118: 0.32373968, 0.29229: -0.2294144, 0.16891578: -0.23945215, 0.29663152: 0.24329278, 0.24622414: 0.23044848, 0.24229902: 0.32042518, -0.2766126: -0.054901045, 0.15617745: 0.31474933, -0.24385023: -0.053700574, -0.30587882: -0.24206749, -0.21471393: -0.16665097, -0.2332125: -0.25529802, 0.23722565: 0.097756356, -0.3429638: -0.36734593, -0.2518457: -0.13961582, -0.16369581: -0.29142535, -0.24825032: -0.25737184, 0.008562422: 0.33578026, -0.20339835: -0.17866889, 0.27510154: 0.21218002, -0.04700012: -0.20413424, 0.27510226: 0.18628381, -0.26729766: -0.2808587, -0.1606115: -0.16901131, -0.05640486: -0.2817691, 0.2840343: -0.03534875, 0.14236687: 0.30950817, 0.24853541: 0.18124965, -0.29303244: -0.32381275, -0.3230352: -0.332122, 0.3255388: 0.3351656, -0.218618: -0.1828706, -0.33096313: -0.30193853, -0.30111656: -0.2630765, -0.2789692: -0.320986, -0.10665599: 0.23200609, -0.07999312: -0.0929607, -0.1291688: 0.3225833, -0.08602957: -0.027050752, 0.23919384: 0.22573857, -0.2674705: -0.31467068, 0.14899494: 0.3350387, -0.15921152: 0.02642262, 0.031970903: 0.26998943, -0.03653883: 0.111082815, -0.1804031: -0.3151737, 0.34596273: 0.30073348, 0.22255833: 0.3254803, 0.19012347: 0.304511, 0.07912842: -0.16180144, -0.16496043: 0.10501947, -0.081468835: 0.028283482, -0.16007361: 0.18061556, -0.2041476: -0.20077772, 0.30663407: 0.28863305, 0.23967546: 0.30364993, -0.20116329: 0.16441107, 0.26600322: 0.25395703, -0.24105866: 0.101792686, 0.27178204: 0.20884143, -0.21928391: -0.34332457, -0.36608636: -0.32074517, -0.091544814: -0.27153912, -0.31652567: 0.24650156, -0.03517492: 0.24807611, -0.25837722: 0.11468564, 0.3235401: 0.13369858, 0.26127118: 0.20933014, -0.28667277: 0.12451834, 0.32871392: -0.03105042, 0.32250276: 0.078584656, -0.04699974: 0.43024912, -0.27694234: -0.03148326, -0.31135282: -0.07347326, -0.25963026: -0.01418014, -0.017305462: -0.29846865, 0.2597632: -0.050257545, -0.27099314: -0.05146012, 0.19347605: -0.2172227, 0.26791844: 0.15309343, 0.36839586: -0.1563612, -0.21523708: -0.19217677, 0.27922323: -0.057690807, 0.27373746: -0.16617326, 0.028865002: -0.32652763, 0.39479724: -0.17323445, -0.0557381: -0.15932721, 0.23507422: -0.030975098, -0.28842267: 0.020788206, -0.122801416: 0.031890653, 0.17231111: -0.25526586, 0.20647478: -0.32236165, 0.29367664: -0.059805162, 0.26333636: 0.2240187, 0.26602906: -0.23424031, -0.3065478: 0.27276894, 0.23547257: -0.2836596, 0.251031: 0.27864462, -0.025077946: -0.1570172, -0.16743335: -0.2702463, 0.33643693: 0.04306456, 0.2573422: 0.1129313, 0.2574595: 0.24052635, -0.1979039: -0.015028264, 0.19687954: 0.24332103, -0.29728445: 0.17198709, 0.312534: 0.18751568, -0.2741623: 0.07780686, -0.24084252: -0.0069385422, 0.30148944: 0.23297043, 0.2975968: 0.23942555, 0.23948485: 0.2988736, 0.31892452: -0.29276085, 0.35108173: 0.29785374, 0.030993335: -0.2502092, -0.04933478: -0.10225806, 0.30126247: -0.28601453, -0.17067245: 0.34101123, 0.2805346: -0.282124, 0.3486572: -0.32925478, 0.2491178: -0.2291511, 0.17505725: -0.2914126, 0.042424377: -0.34177622, 0.2279788: 0.29655725, 0.20055863: 0.21768512, 0.27957395: -0.26790965, -0.28614965: -0.028478406, 0.2596107: -0.37239498, 0.22903863: 0.22957322, 0.28749314: -0.35492563, 0.27941927: -0.22973679, 0.1882708: 0.2933245, 0.1671929: -0.20350827, 0.30105573: -0.35650945}\n",
            "Loading synonyms.csv...\n",
            "Loaded 1235 synonyms.\n",
            "Starting paraphrasing process...\n",
            "Paraphrasing sentence: The quick brown fox jumped over the huge wall\n",
            "Word 'The' not found in word vectors, keeping it unchanged.\n",
            "Word 'quick' not found in word vectors, keeping it unchanged.\n",
            "Word 'brown' not found in word vectors, keeping it unchanged.\n",
            "Word 'fox' not found in word vectors, keeping it unchanged.\n",
            "Word 'jumped' not found in word vectors, keeping it unchanged.\n",
            "Word 'over' not found in word vectors, keeping it unchanged.\n",
            "Word 'the' not found in word vectors, keeping it unchanged.\n",
            "Word 'huge' not found in word vectors, keeping it unchanged.\n",
            "Word 'wall' not found in word vectors, keeping it unchanged.\n",
            "Final paraphrased sentence: The quick brown fox jumped over the huge wall\n",
            "Original Sentence: The quick brown fox jumped over the huge wall\n",
            "Paraphrased Sentence: The quick brown fox jumped over the huge wall\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import csv\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load trained model\n",
        "model = load_model('paraphrase_model.h5', compile=False)\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "# Load word embeddings\n",
        "file_path = \"/content/word_vectors.csv\"\n",
        "existing_vectors = {}\n",
        "with open(file_path, 'r', encoding='utf-8') as f:\n",
        "    reader = csv.reader(f)\n",
        "    next(reader, None)  # Skip header if present\n",
        "    for row in reader:\n",
        "        if len(row) < 2:\n",
        "            continue  # Skip incomplete rows\n",
        "        word = row[0].lower()  # Convert to lowercase\n",
        "        vector = np.array(list(map(float, row[1:])))\n",
        "        existing_vectors[word] = vector\n",
        "\n",
        "# Load synonyms.csv\n",
        "synonyms_df = pd.read_csv('/content/synonyms (2).csv', header=None, encoding='utf-8')\n",
        "synonyms_df.rename(columns={0: 'Synonym'}, inplace=True)\n",
        "\n",
        "# Extract synonym embeddings into a dictionary\n",
        "synonym_embeddings = np.array(synonyms_df.iloc[:, 1:].values, dtype=np.float32)\n",
        "synonyms = dict(zip(synonyms_df['Synonym'].str.lower(), synonym_embeddings))  # Convert synonyms to lowercase\n",
        "\n",
        "# Function to find the closest synonym\n",
        "def find_closest_synonym(predicted_embedding, synonyms):\n",
        "    closest_word = None\n",
        "    max_similarity = -1\n",
        "\n",
        "    for word, vector in synonyms.items():\n",
        "        similarity = cosine_similarity(predicted_embedding.reshape(1, -1), vector.reshape(1, -1))[0][0]\n",
        "        if similarity > max_similarity:\n",
        "            max_similarity = similarity\n",
        "            closest_word = word\n",
        "\n",
        "    print(f\"Closest synonym found: {closest_word} (Similarity: {max_similarity})\", flush=True)\n",
        "    return closest_word\n",
        "\n",
        "# Function to paraphrase a sentence\n",
        "def paraphrase_sentence(sentence, word_vectors, synonyms, model):\n",
        "    print(f\"Paraphrasing sentence: {sentence}\", flush=True)\n",
        "    words = sentence.split()\n",
        "    paraphrased_sentence = []\n",
        "\n",
        "    for word in words:\n",
        "        word_lower = word.lower()  # Ensure lowercase matching\n",
        "\n",
        "        if word_lower in word_vectors:\n",
        "            # Get parent embedding\n",
        "            parent_embedding = word_vectors[word_lower]\n",
        "            print(f\"Embedding for '{word_lower}': {parent_embedding}\", flush=True)\n",
        "\n",
        "            try:\n",
        "                # Predict synonym embedding\n",
        "                predicted_synonym_embedding = model.predict(np.array([parent_embedding]))[0][0]\n",
        "                print(f\"Predicted embedding for '{word_lower}': {predicted_synonym_embedding}\", flush=True)\n",
        "\n",
        "                # Find closest synonym\n",
        "                closest_synonym = find_closest_synonym(predicted_synonym_embedding, synonyms)\n",
        "\n",
        "                # Replace with synonym if found\n",
        "                paraphrased_sentence.append(closest_synonym if closest_synonym else word)\n",
        "            except Exception as e:\n",
        "                print(f\"Error predicting synonym for '{word_lower}': {e}\", flush=True)\n",
        "                paraphrased_sentence.append(word)  # Keep original word on failure\n",
        "        else:\n",
        "            print(f\"Word '{word_lower}' not found in word vectors, keeping it unchanged.\", flush=True)\n",
        "            paraphrased_sentence.append(word)\n",
        "\n",
        "    paraphrased_text = ' '.join(paraphrased_sentence)\n",
        "    print(f\"Final paraphrased sentence: {paraphrased_text}\", flush=True)\n",
        "    return paraphrased_text\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Starting paraphrasing process...\", flush=True)\n",
        "\n",
        "    # Input sentence\n",
        "    sentence = \"The bank is near the bank of a river\"\n",
        "\n",
        "    # Paraphrase the sentence\n",
        "    paraphrased_sentence = paraphrase_sentence(sentence, existing_vectors, synonyms, model)\n",
        "\n",
        "    print(f'Original Sentence: {sentence}', flush=True)\n",
        "    print(f'Paraphrased Sentence: {paraphrased_sentence}', flush=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UxHWIXmKS6Z3",
        "outputId": "7faff253-8ea3-4408-bac0-0d5e5769cbfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting paraphrasing process...\n",
            "Paraphrasing sentence: The bank is near the bank of a river\n",
            "Word 'the' not found in word vectors, keeping it unchanged.\n",
            "Embedding for 'bank': [ 0.31178635  0.055164    0.31884974  0.319938    0.07515985 -0.19541635\n",
            "  0.3035846  -0.30206776 -0.30270803  0.29795843 -0.03657262 -0.27057752\n",
            "  0.1260939  -0.0144297   0.16070908 -0.29281753 -0.16562846  0.055658\n",
            "  0.17965874  0.17990425]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
            "Predicted embedding for 'bank': [ 0.21307668  0.0778587   0.22855344  0.20203245  0.04104276 -0.15749526\n",
            "  0.21100959 -0.20532925 -0.26424992  0.26136678 -0.0378166  -0.29118901\n",
            "  0.10810449 -0.00463709  0.1972131  -0.31828505 -0.15027335  0.04805079\n",
            "  0.14756846  0.2062249 ]\n",
            "Closest synonym found: banking (Similarity: 0.9457366466522217)\n",
            "Word 'is' not found in word vectors, keeping it unchanged.\n",
            "Embedding for 'near': [ 0.11912069  0.22036478  0.1379675   0.23451321 -0.28102046 -0.17728224\n",
            "  0.29865184 -0.20424607 -0.2960768  -0.05971853  0.19114517 -0.03597821\n",
            " -0.03153485 -0.33508435 -0.06653225 -0.32231724  0.27781767 -0.34488362\n",
            "  0.28546718  0.27020037]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "Predicted embedding for 'near': [ 0.098923    0.16696186  0.0794262   0.184659   -0.17863806 -0.10211571\n",
            "  0.1848973  -0.10581936 -0.19875802 -0.01643225  0.1799228  -0.02564299\n",
            " -0.04903675 -0.33164287 -0.05713102 -0.2998787   0.27710205 -0.33420494\n",
            "  0.27495262  0.26229864]\n",
            "Closest synonym found: outskirts (Similarity: 0.9681336283683777)\n",
            "Word 'the' not found in word vectors, keeping it unchanged.\n",
            "Embedding for 'bank': [ 0.31178635  0.055164    0.31884974  0.319938    0.07515985 -0.19541635\n",
            "  0.3035846  -0.30206776 -0.30270803  0.29795843 -0.03657262 -0.27057752\n",
            "  0.1260939  -0.0144297   0.16070908 -0.29281753 -0.16562846  0.055658\n",
            "  0.17965874  0.17990425]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "Predicted embedding for 'bank': [ 0.21307668  0.0778587   0.22855344  0.20203245  0.04104276 -0.15749526\n",
            "  0.21100959 -0.20532925 -0.26424992  0.26136678 -0.0378166  -0.29118901\n",
            "  0.10810449 -0.00463709  0.1972131  -0.31828505 -0.15027335  0.04805079\n",
            "  0.14756846  0.2062249 ]\n",
            "Closest synonym found: banking (Similarity: 0.9457366466522217)\n",
            "Embedding for 'of': [ 0.26745003  0.32078758  0.19152468  0.2131108  -0.32614934 -0.21137668\n",
            "  0.34340933 -0.1930709  -0.28818345 -0.00188747  0.18933602 -0.12832761\n",
            " -0.10368827 -0.23393482  0.19724916 -0.26730886  0.13270798 -0.2254129\n",
            "  0.28602964  0.26975164]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "Predicted embedding for 'of': [ 0.1512265   0.18779504  0.21949911  0.15793832 -0.18793088 -0.07992224\n",
            "  0.19200873 -0.123661   -0.21933731 -0.01358858  0.18556102 -0.15652652\n",
            " -0.11353009 -0.22290218  0.20858732 -0.26660705  0.14176416 -0.22489312\n",
            "  0.28677738  0.21864627]\n",
            "Closest synonym found: away (Similarity: 0.8878275156021118)\n",
            "Word 'a' not found in word vectors, keeping it unchanged.\n",
            "Embedding for 'river': [ 0.41991436  0.3410992  -0.04055678  0.16062616 -0.05863952 -0.2522959\n",
            " -0.31900185 -0.05511096 -0.2423671   0.24504939  0.38004208  0.01539854\n",
            "  0.07157111  0.37683076 -0.4150108   0.15778005  0.3280682  -0.4767155\n",
            " -0.2789692  -0.320986  ]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "Predicted embedding for 'river': [ 0.25313812  0.17950629 -0.01981437  0.04593492 -0.04892343 -0.11116382\n",
            " -0.20358486 -0.00290386 -0.14336653  0.18075201  0.3855678  -0.00784432\n",
            "  0.03202739  0.39370716 -0.4151621   0.11892343  0.31680286 -0.4314135\n",
            " -0.31867033 -0.2905398 ]\n",
            "Closest synonym found: rivers (Similarity: 0.960978090763092)\n",
            "Final paraphrased sentence: The banking is outskirts the banking away a rivers\n",
            "Original Sentence: The bank is near the bank of a river\n",
            "Paraphrased Sentence: The banking is outskirts the banking away a rivers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ev-ln44nWP7U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}